# Assistant RAG Multi-Mod√®les 

Ce projet est une application Streamlit pour un assistant de r√©cup√©ration de documents (RAG) multi-mod√®les utilisant des mod√®les de langage LLM. L'application permet de charger, traiter et interroger des documents en utilisant diff√©rents mod√®les de langage.

[![Documentation Status](https://readthedocs.org/projects/multi-model-rag-assistant-documentation/badge/?version=latest)](https://multi-model-rag-assistant-documentation.readthedocs.io/fr/latest/)

## Documentation

La documentation compl√®te du projet est disponible ici :  
üëâ [Consultez la documentation en ligne](https://multi-model-rag-assistant-documentation.readthedocs.io/fr/latest/)

---

## Fonctionnalit√©s

- *Chargement de documents* : Supporte les fichiers PDF, TXT et DOCX.
- *Traitement des documents* : Utilise des loaders sp√©cifiques pour chaque type de fichier.
- *Vectorisation des documents* : Utilise Chroma pour cr√©er un vectorstore persistant.
- *Recherche et r√©cup√©ration* : Utilise des cha√Ænes de r√©cup√©ration conversationnelle pour interroger les documents.
- *Interface utilisateur* : Interface intuitive avec Streamlit pour configurer et interagir avec l'assistant.

## Installation

### Pr√©requis

Assurez-vous d'avoir install√© les √©l√©ments suivants :

- [Python 3.8+](https://www.python.org/downloads/)
- [pip](https://pip.pypa.io/en/stable/installation/)
- [Git](https://git-scm.com/)

### √âtapes d'installation

1. Clonez le d√©p√¥t :
    ```bash
    git clone https://github.com/votre-utilisateur/assistant-rag-multi-modeles.git
    cd assistant-rag-multi-modeles
    ```

2. Cr√©ez un environnement virtuel (recommand√©) :
    ```bash
    python -m venv env
    source env/bin/activate  # Sur Windows, utilisez `env\Scripts\activate`
    ```

3. Installez les d√©pendances :
    ```bash
    pip install -r requirements.txt
    ```

4. Lancez l'application Streamlit :
    ```bash
    streamlit run code_app.py
    ```

## Utilisation

1. Lancez l'application Streamlit :
    ```bash
    streamlit run code_app.py
    ```

2. Configurez l'assistant via la barre lat√©rale :
    - Choisissez le mod√®le LLM.
    - R√©glez le seuil de pertinence.
    - Chargez les documents √† traiter.

3. Posez vos questions dans la zone de saisie et obtenez des r√©ponses pertinentes bas√©es sur les documents charg√©s.

## Configuration

- *Mod√®les support√©s* : llama3.1, mistral, llama3.2, gemma:7b, llama2:13b
- *Seuil de pertinence* : Ajustable via un slider dans l'interface.

## Interface Utilisateur

Voici quelques captures d'√©cran pour illustrer le fonctionnement de l'application :

![Chargement des Documents](images/image_1.jpg)
![Chargement des Documents](images/image_2.jpg)
![Chargement des Documents](images/image_3.jpg)

## Contribuer

Les contributions sont les bienvenues ! Veuillez soumettre une pull request ou ouvrir une issue pour discuter des changements que vous souhaitez apporter.

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.
